{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate mean Intersection-Over-Union (mIOU) metric\n",
    "\n",
    "A ready-to-use script to find mean Intersection-Over-Union metric of class pairs\n",
    "\n",
    "\n",
    "**Input**:\n",
    "- Existing Project (i.e. \"london_roads\")\n",
    "- At least one pair of classes (i.e. (\"car_gt\", \"car_lb\"))\n",
    "\n",
    "**Output**:\n",
    "- intersection, union and IoU for each class pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervisely_lib as sly\n",
    "import os\n",
    "import collections\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Edit the following settings for your own case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this field to the name of your team, where target workspace exists.\n",
    "team_name = \"jupyter_tutorials\"\n",
    "# Change this field to the of your workspace, where target project exists.\n",
    "workspace_name = \"metrics_tutorials\"\n",
    "# Change this field to the name of your target project.\n",
    "project_name = \"tutorial_metric_iou_project\"\n",
    "\n",
    "# Configure the following dictionary  so that is will match pairs of ground truth and predicted classes\n",
    "# between which IOU will be caluclated.\n",
    "classes_mapping = {\n",
    "    \"dog\": \"annotator_dog\",\n",
    "    \"person\": \"annotator_person\",    \n",
    "}\n",
    "\n",
    "# If you are running this notebook on a Supervisely web instance, the connection\n",
    "# details below will be filled in from environment variables automatically.\n",
    "#\n",
    "# If you are running this notebook locally on your own machine, edit to fill in the\n",
    "# connection details manually. You can find your access token at\n",
    "# \"Your name on the top right\" -> \"Account settings\" -> \"API token\".\n",
    "address = os.environ['SERVER_ADDRESS']\n",
    "token = os.environ['API_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script setup\n",
    "\n",
    "Import nessesary packages and initialize Supervisely API to remotely manage your projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API object\n",
    "api = sly.Api(address, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify input values\n",
    "\n",
    "Test that context (team / workspace / project) exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: id=8, name=dima\n",
      "Workspace: id=14, name=First Workspace\n",
      "Project: id=34, name=tutorial_metric_iou_project\n"
     ]
    }
   ],
   "source": [
    "team = api.team.get_info_by_name(team_name)\n",
    "if team is None:\n",
    "    raise RuntimeError(\"Team {!r} not found\".format(team_name))\n",
    "\n",
    "workspace = api.workspace.get_info_by_name(team.id, workspace_name)\n",
    "if workspace is None:\n",
    "    raise RuntimeError(\"Workspace {!r} not found\".format(workspace_name))\n",
    "    \n",
    "project = api.project.get_info_by_name(workspace.id, project_name)\n",
    "if project is None:\n",
    "    raise RuntimeError(\"Project {!r} not found\".format(project_name))\n",
    "    \n",
    "print(\"Team: id={}, name={}\".format(team.id, team.name))\n",
    "print(\"Workspace: id={}, name={}\".format(workspace.id, workspace.name))\n",
    "print(\"Project: id={}, name={}\".format(project.id, project.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Meta of Source Project\n",
    "\n",
    "Project Meta contains information about classes and tags# Get source project meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_json = api.project.get_meta(project.id)\n",
    "meta = sly.ProjectMeta.from_json(meta_json)\n",
    "\n",
    "# check that all classes exist\n",
    "project_classes_names = list(classes_mapping.keys()) + list(classes_mapping.values())\n",
    "\n",
    "for class_name in project_classes_names:\n",
    "    if class_name not in meta.obj_classes.keys():\n",
    "        raise RuntimeError(\"Class {!r} not found in source project {!r}\".format(class_name, project.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metric evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_iou = sly.IoUMetric(classes_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over all images, and calculate metric by annotations pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: project = 'tutorial_metric_iou_project', dataset = 'dataset_1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process annotations: 100%|██████████| 3/3 [00:00<00:00, 44.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: project = 'tutorial_metric_iou_project', dataset = 'dataset_2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Process annotations: 100%|██████████| 2/2 [00:00<00:00, 65.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for dataset in api.dataset.get_list(project.id):\n",
    "    \n",
    "    # generate dataset name in destination project if it exists\n",
    "    print(\"Processing: project = {!r}, dataset = {!r}\".format(project.name, dataset.name), flush=True)\n",
    "    \n",
    "    images = api.image.get_list(dataset.id)\n",
    "    with tqdm(total=len(images), desc=\"Process annotations\") as progress_bar:\n",
    "        for batch in sly.batched(images):\n",
    "            image_ids = [image_info.id for image_info in batch]\n",
    "            ann_infos = api.annotation.download_batch(dataset.id, image_ids)\n",
    "            \n",
    "            for ann_info in ann_infos:\n",
    "                ann = sly.Annotation.from_json(ann_info.annotation, meta)\n",
    "                # We are using the same annotation on the both side of the metric computation\n",
    "                # (classes_mapping provides the corresponding classes that we will look for\n",
    "                # in the annotation), but it is also possible to use different annotations\n",
    "                # on left and right, e.g. to compare the source hand-labeled project to a\n",
    "                # neural netork inference result.\n",
    "                metric_iou.add_pair(ann, ann)\n",
    "            \n",
    "            progress_bar.update(len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results by default logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"message\": \"**************** Result IoU metric values ****************\", \"timestamp\": \"2019-04-16T16:12:52.648Z\", \"level\": \"info\"}\n",
      "{\"message\": \"NOTE! Values for \\\"intersection\\\" and \\\"union\\\" are in pixels.\", \"timestamp\": \"2019-04-16T16:12:52.651Z\", \"level\": \"info\"}\n",
      "{\"message\": \"1. Classes dog <-> annotator_dog:   IoU = 0.870172,  mean intersection = 61491.400000, mean union = 70665.800000\", \"timestamp\": \"2019-04-16T16:12:52.654Z\", \"level\": \"info\"}\n",
      "{\"message\": \"2. Classes person <-> annotator_person:   IoU = 0.448813,  mean intersection = 53688.400000, mean union = 119623.200000\", \"timestamp\": \"2019-04-16T16:12:52.656Z\", \"level\": \"info\"}\n",
      "{\"message\": \"Total:   IoU = 0.605289,  mean intersection = 575899.000000, mean union = 951445.000000\", \"timestamp\": \"2019-04-16T16:12:52.658Z\", \"level\": \"info\"}\n"
     ]
    }
   ],
   "source": [
    "metric_iou.log_total_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------------------------+\n",
      "|         classes pair        |      metrics values      |\n",
      "+-----------------------------+--------------------------+\n",
      "|    dog <-> annotator_dog    |  intersection: 61491.4   |\n",
      "|                             |      union: 70665.8      |\n",
      "|                             | iou: 0.8701719926753819  |\n",
      "|                             |                          |\n",
      "| person <-> annotator_person |  intersection: 53688.4   |\n",
      "|                             |     union: 119623.2      |\n",
      "|                             | iou: 0.44881260491275937 |\n",
      "|                             |                          |\n",
      "|            TOTAL            |   intersection: 575899   |\n",
      "|                             |      union: 951445       |\n",
      "|                             | iou: 0.6052887975658078  |\n",
      "|                             |                          |\n",
      "+-----------------------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Metrics for each pair of classes separately.\n",
    "results = metric_iou.get_metrics()\n",
    "\n",
    "# Metrics aggregated over all pairs of classes from classes_mapping\n",
    "total_results = metric_iou.get_total_metrics()\n",
    "\n",
    "table = PrettyTable([\"classes pair\", \"metrics values\"])\n",
    "\n",
    "def build_values_text(values):\n",
    "    values_text = \"\"\n",
    "    for metrics_name, value in values.items():\n",
    "        values_text += \"{}: {}\\n\".format(metrics_name, value)\n",
    "    return values_text\n",
    "    \n",
    "for first_pair_class, values in results.items():\n",
    "    pair_text = \"{} <-> {}\".format(first_pair_class, classes_mapping[first_pair_class])\n",
    "    table.add_row([pair_text, build_values_text(values)])\n",
    "\n",
    "table.add_row([\"TOTAL\", build_values_text(total_results)])\n",
    "print(table.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
