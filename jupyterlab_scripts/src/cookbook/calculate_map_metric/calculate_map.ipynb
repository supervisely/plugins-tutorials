{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate mean average presicion (mAP) metric\n",
    "\n",
    "A ready-to-use script to compute mean average precision (mAP) metric.\n",
    "\n",
    "**Input**:\n",
    "- Existing project annotated with both ground truth and predicted objects. The predicted objects must be labeled with `confidence` tags with values equal to the model prediction confidence.\n",
    "- At least one pair of corresponding ground truth and prediction class names, e.g. (\"person\", \"person_predicted\").\n",
    "\n",
    "**Output**:\n",
    "- Mean average precision for each corresponding class pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervisely_lib as sly\n",
    "import os\n",
    "import collections\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "from supervisely_lib.metric.map_metric import AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Edit the following settings for your own case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this field to the name of your team, where target workspace exists.\n",
    "team_name = \"jupyter_tutorials\"\n",
    "\n",
    "# Change this field to the of your workspace, where target project exists.\n",
    "workspace_name = \"metrics_tutorials\"\n",
    "\n",
    "# Change this field to the name of your target project.\n",
    "project_name = \"map_metric_demo_project\"\n",
    "\n",
    "# Configure the following dictionary so that is will match pairs of ground truth and predicted classes\n",
    "# for which the metrics will be caluclated.\n",
    "classes_mapping = {\n",
    "    \"bike\": \"motorbike_pred\",\n",
    "    \"dog\": \"dog_pred\",\n",
    "    \"person\": \"person_pred\",\n",
    "}\n",
    "\n",
    "# Minimum intersection over uinon value for which to overlapping objects will be\n",
    "# considered to have matched. Increase to only take close matches into account;\n",
    "# decrease to also consider less significant overlaps.\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# If you are running this notebook on a Supervisely web instance, the connection\n",
    "# Edit those values if you run this notebook on your own PC\n",
    "# details below will be filled in from environment variables automatically.\n",
    "#\n",
    "# If you are running this notebook locally on your own machine, edit to fill in the\n",
    "# connection details manually. You can find your access token at\n",
    "# \"Your name on the top right\" -> \"Account settings\" -> \"API token\".\n",
    "address = os.environ['SERVER_ADDRESS']\n",
    "token = os.environ['API_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script setup\n",
    "\n",
    "Initialize Supervisely API to remotely manage your projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API object\n",
    "api = sly.Api(address, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify input values\n",
    "\n",
    "Test that context (team / workspace / project) exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: id=3, name=jupyter_tutorials\n",
      "Workspace: id=10, name=metrics_tutorials\n",
      "Project: id=401, name=map_metric_demo_project\n"
     ]
    }
   ],
   "source": [
    "team = api.team.get_info_by_name(team_name)\n",
    "if team is None:\n",
    "    raise RuntimeError(\"Team {!r} not found\".format(team_name))\n",
    "\n",
    "workspace = api.workspace.get_info_by_name(team.id, workspace_name)\n",
    "if workspace is None:\n",
    "    raise RuntimeError(\"Workspace {!r} not found\".format(workspace_name))\n",
    "    \n",
    "project = api.project.get_info_by_name(workspace.id, project_name)\n",
    "if project is None:\n",
    "    raise RuntimeError(\"Project {!r} not found\".format(project_name))\n",
    "    \n",
    "print(\"Team: id={}, name={}\".format(team.id, team.name))\n",
    "print(\"Workspace: id={}, name={}\".format(workspace.id, workspace.name))\n",
    "print(\"Project: id={}, name={}\".format(project.id, project.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get source project meta\n",
    "\n",
    "Project meta contains information about classes and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_json = api.project.get_meta(project.id)\n",
    "meta = sly.ProjectMeta.from_json(meta_json)\n",
    "\n",
    "# check that all classes exist\n",
    "project_classes_names = list(classes_mapping.keys()) + list(classes_mapping.values())\n",
    "\n",
    "for class_name in project_classes_names:\n",
    "    if not meta.obj_classes.has_key(class_name):\n",
    "        raise RuntimeError(\"Class {!r} not found in source project {!r}\".format(class_name, project.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metric evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_evaluator = sly.MAPMetric(classes_mapping, iou_threshold, confidence_tag_name='confidence_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over all images, and calculate metric by annotations pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: project = 'map_metric_demo_project', dataset = 'dataset_02'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process annotations: 100%|██████████| 2/2 [00:00<00:00, 48.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: project = 'map_metric_demo_project', dataset = 'dataset_01'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Process annotations: 100%|██████████| 3/3 [00:00<00:00, 21.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for dataset in api.dataset.get_list(project.id):\n",
    "    \n",
    "    # generate dataset name in destination project if it exists\n",
    "    print(\"Processing: project = {!r}, dataset = {!r}\".format(project.name, dataset.name), flush=True)\n",
    "    \n",
    "    images = api.image.get_list(dataset.id)\n",
    "    with tqdm(total=len(images), desc=\"Process annotations\") as progress_bar:\n",
    "        for batch in sly.batched(images):\n",
    "            image_ids = [image_info.id for image_info in batch]\n",
    "            ann_infos = api.annotation.download_batch(dataset.id, image_ids)\n",
    "            \n",
    "            for ann_info in ann_infos:\n",
    "                ann = sly.Annotation.from_json(ann_info.annotation, meta)\n",
    "                # We are using the same annotation on the both side of the metric computation\n",
    "                # (classes_mapping provides the corresponding classes that we will look for\n",
    "                # in the annotation), but it is also possible to use different annotations\n",
    "                # on left and right, e.g. to compare the source hand-labeled project to a\n",
    "                # neural netork inference result.\n",
    "                map_evaluator.add_pair(ann, ann)\n",
    "            \n",
    "            progress_bar.update(len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results with default logger\n",
    "\n",
    "The results are logged with the default Supervisely logger, so that the same code can be used in any custom plugin, and the log output would be nicely formatted in the task log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"message\": \"                                                                                \", \"timestamp\": \"2019-04-24T14:48:32.764Z\", \"level\": \"info\"}\n",
      "{\"message\": \"***************** Result metrics values for 0.5 IoU threshold ******************\", \"timestamp\": \"2019-04-24T14:48:32.768Z\", \"level\": \"info\"}\n",
      "{\"message\": \"Start evaluation of macro metrics.\", \"timestamp\": \"2019-04-24T14:48:32.769Z\", \"level\": \"info\"}\n",
      "{\"message\": \"Finish macro evaluation\", \"timestamp\": \"2019-04-24T14:48:32.770Z\", \"level\": \"info\"}\n",
      "{\"message\": \"                                                                                \", \"timestamp\": \"2019-04-24T14:48:32.771Z\", \"level\": \"info\"}\n",
      "{\"message\": \"*********** Results for pair of classes <<bike <-> motorbike_pred>>  ***********\", \"timestamp\": \"2019-04-24T14:48:32.771Z\", \"level\": \"info\"}\n",
      "{\"message\": \"Average Precision (AP): 0.6363636363636364\", \"timestamp\": \"2019-04-24T14:48:32.772Z\", \"level\": \"info\"}\n",
      "{\"message\": \"                                                                                \", \"timestamp\": \"2019-04-24T14:48:32.772Z\", \"level\": \"info\"}\n",
      "{\"message\": \"************** Results for pair of classes <<dog <-> dog_pred>>  ***************\", \"timestamp\": \"2019-04-24T14:48:32.773Z\", \"level\": \"info\"}\n",
      "{\"message\": \"Average Precision (AP): 1.0\", \"timestamp\": \"2019-04-24T14:48:32.774Z\", \"level\": \"info\"}\n",
      "{\"message\": \"                                                                                \", \"timestamp\": \"2019-04-24T14:48:32.774Z\", \"level\": \"info\"}\n",
      "{\"message\": \"*********** Results for pair of classes <<person <-> person_pred>>  ************\", \"timestamp\": \"2019-04-24T14:48:32.775Z\", \"level\": \"info\"}\n",
      "{\"message\": \"Average Precision (AP): 0.6363636363636364\", \"timestamp\": \"2019-04-24T14:48:32.776Z\", \"level\": \"info\"}\n",
      "{\"message\": \"                                                                                \", \"timestamp\": \"2019-04-24T14:48:32.777Z\", \"level\": \"info\"}\n",
      "{\"message\": \"***************************** Mean metrics values ******************************\", \"timestamp\": \"2019-04-24T14:48:32.777Z\", \"level\": \"info\"}\n",
      "{\"message\": \"Mean Average Precision (mAP): 0.7575757575757575\", \"timestamp\": \"2019-04-24T14:48:32.778Z\", \"level\": \"info\"}\n",
      "{\"message\": \"                                                                                \", \"timestamp\": \"2019-04-24T14:48:32.778Z\", \"level\": \"info\"}\n"
     ]
    }
   ],
   "source": [
    "map_evaluator.log_total_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------------------------+\n",
      "|       classes pair      |             metrics values            |\n",
      "+-------------------------+---------------------------------------+\n",
      "| bike <-> motorbike_pred | average-precision: 0.6363636363636364 |\n",
      "|                         |                                       |\n",
      "|     dog <-> dog_pred    |         average-precision: 1.0        |\n",
      "|                         |                                       |\n",
      "|  person <-> person_pred | average-precision: 0.6363636363636364 |\n",
      "|                         |                                       |\n",
      "|          TOTAL          | average-precision: 0.7575757575757575 |\n",
      "+-------------------------+---------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Metrics for each pair of classes separately.\n",
    "results = map_evaluator.get_metrics()\n",
    "\n",
    "# Metrics aggregated over all pairs of classes from classes_mapping\n",
    "total_results = map_evaluator.get_total_metrics()\n",
    "\n",
    "table = PrettyTable([\"classes pair\", \"metrics values\"])\n",
    "\n",
    "def build_values_text(values):\n",
    "    return ''.join(\n",
    "        \"{}: {}\\n\".format(metrics_name, value)\n",
    "        for metrics_name, value in values.items())\n",
    "    \n",
    "for gt_class, metric_values in results.items():\n",
    "    pair_text = \"{} <-> {}\".format(gt_class, classes_mapping[gt_class])\n",
    "    table.add_row([pair_text, build_values_text(metric_values)])\n",
    "\n",
    "table.add_row([\"TOTAL\", \"average-precision: {}\".format(total_results[AP]) ])\n",
    "print(table.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
