{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference task (id=925) started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"message\": \"Project '_model_eval__391__104' already exists, reusing it and skipping inference.\", \"timestamp\": \"2019-06-13T19:27:43.394Z\", \"level\": \"warn\"}\n",
      "{\"message\": \"Project '_model_eval__393__103' already exists, reusing it and skipping inference.\", \"timestamp\": \"2019-06-13T19:27:43.437Z\", \"level\": \"warn\"}\n",
      "{\"message\": \"Project '_model_eval__393__104' already exists, reusing it and skipping inference.\", \"timestamp\": \"2019-06-13T19:27:43.467Z\", \"level\": \"warn\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference task (id=925) finished\n",
      "Overall metrics:\n",
      "+-----------------+---------------------+--------------------+\n",
      "|      Models     |         IoU         |        mAP         |\n",
      "+-----------------+---------------------+--------------------+\n",
      "|  yolo_2_epochs  | 0.13149535709937687 | 0.2121212121212121 |\n",
      "| yolo_200_epochs |  0.6669136976983602 |        1.0         |\n",
      "+-----------------+---------------------+--------------------+\n",
      "Metrics for project 'lemons_annotated':\n",
      "+-----------------+---------------------+--------------------+\n",
      "|      Models     |         IoU         |        mAP         |\n",
      "+-----------------+---------------------+--------------------+\n",
      "|  yolo_2_epochs  | 0.13149535709937687 | 0.2121212121212121 |\n",
      "| yolo_200_epochs |  0.6669136976983602 |        1.0         |\n",
      "+-----------------+---------------------+--------------------+\n",
      "Metrics for project 'lemons_annotated_copy':\n",
      "+-----------------+---------------------+--------------------+\n",
      "|      Models     |         IoU         |        mAP         |\n",
      "+-----------------+---------------------+--------------------+\n",
      "|  yolo_2_epochs  | 0.13149535709937687 | 0.2121212121212121 |\n",
      "| yolo_200_epochs |  0.6669136976983602 |        1.0         |\n",
      "+-----------------+---------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "import supervisely_lib as sly\n",
    "\n",
    "from supervisely_lib.metric.iou_metric import IOU\n",
    "from supervisely_lib.metric.map_metric import AP\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "MetricInfo = collections.namedtuple('MetricInfo', 'metric_factory metric_id_with_title')\n",
    "\n",
    "map_iou_threshold = 0.1\n",
    "\n",
    "PREDICTION_SUFFIX = '_pred'\n",
    "\n",
    "metric_infos = [\n",
    "    MetricInfo(metric_factory=lambda _classes_mapping: sly.IoUMetric(_classes_mapping),\n",
    "               metric_id_with_title={IOU: 'IoU'}),\n",
    "    MetricInfo(metric_factory=lambda _classes_mapping: sly.MAPMetric(_classes_mapping,\n",
    "                                                                     iou_threshold=map_iou_threshold,\n",
    "                                                                     confidence_tag_name=(\n",
    "                                                                             'confidence' + PREDICTION_SUFFIX)),\n",
    "               metric_id_with_title={AP: 'mAP'}),\n",
    "]\n",
    "\n",
    "team_name = \"jupyter_tutorials\"\n",
    "workspace_name = \"metrics_tutorials\"\n",
    "agent_name = \" Skinny Alpaca\"\n",
    "\n",
    "model_names = ['yolo_2_epochs', 'yolo_200_epochs']\n",
    "project_names = ['lemons_annotated', 'lemons_annotated_copy']\n",
    "\n",
    "inference_config = {\n",
    "    'mode': {\n",
    "        'model_classes': {\n",
    "            'add_suffix': PREDICTION_SUFFIX\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "classes_mapping = {\n",
    "    'kiwi': 'kiwi_bbox' + PREDICTION_SUFFIX,\n",
    "    'lemon': 'lemon_bbox' + PREDICTION_SUFFIX\n",
    "}\n",
    "\n",
    "# ------------- End user settings -------------\n",
    "\n",
    "address = os.environ['SERVER_ADDRESS']\n",
    "token = os.environ['API_TOKEN']\n",
    "api = sly.Api(address, token)\n",
    "\n",
    "team = api.team.get_info_by_name(team_name)\n",
    "if team is None:\n",
    "    raise RuntimeError(f\"Team {team_name!r} not found\")\n",
    "\n",
    "workspace = api.workspace.get_info_by_name(team.id, workspace_name)\n",
    "if workspace is None:\n",
    "    raise RuntimeError(f\"Workspace {workspace_name!r} not found\")\n",
    "\n",
    "agent = api.agent.get_info_by_name(team.id, agent_name)\n",
    "if agent is None:\n",
    "    raise RuntimeError(\"Agent {!r} not found\".format(agent_name))\n",
    "if agent.status is api.agent.Status.WAITING:\n",
    "    raise RuntimeError(\"Agent {!r} is not running\".format(agent_name))\n",
    "\n",
    "model_infos = [api.model.get_info_by_name(workspace.id, model_name) for model_name in model_names]\n",
    "project_infos = [api.project.get_info_by_name(workspace.id, project_name) for project_name in project_names]\n",
    "\n",
    "for model_name, model_info in zip(model_names, model_infos):\n",
    "    if model_info is None:\n",
    "        raise RuntimeError(f'Model {model_name!r} not found.')\n",
    "for project_name, project_info in zip(project_names, project_infos):\n",
    "    if project_info is None:\n",
    "        raise RuntimeError(f'Model {project_name!r} not found.')\n",
    "\n",
    "inference_project_names = {\n",
    "    (project_idx, model_idx): '__'.join(['_model_eval', str(project_info.id), str(model_info.id)])\n",
    "    for project_idx, project_info in enumerate(project_infos)\n",
    "    for model_idx, model_info in enumerate(model_infos)}\n",
    "\n",
    "for project_idx, project_info in enumerate(project_infos):\n",
    "    for model_idx, model_info in enumerate(model_infos):\n",
    "        inference_project_name = inference_project_names[project_idx, model_idx]\n",
    "        inference_project_info = api.project.get_info_by_name(workspace.id, inference_project_name)\n",
    "        if inference_project_info is not None:\n",
    "            sly.logger.warn(f'Project {inference_project_name!r} already exists, reusing it and skipping inference.')\n",
    "            # Inference project already exists. Assume that it has the correct inference data from a previous run.\n",
    "            # Log a warning here.\n",
    "            continue\n",
    "\n",
    "        task_id = api.task.run_inference(\n",
    "            agent.id, project_info.id, model_info.id, inference_project_name, inference_config=inference_config)\n",
    "        print('Inference task (id={}) started'.format(task_id))\n",
    "        api.task.wait(task_id, api.task.Status.FINISHED)\n",
    "        print('Inference task (id={}) finished'.format(task_id))\n",
    "\n",
    "def _make_metric_calculators(metric_infos, model_names, classes_mapping):\n",
    "    return [[metric_info.metric_factory(_classes_mapping=classes_mapping)\n",
    "             for metric_info in metric_infos] for _ in model_names]\n",
    "\n",
    "overall_metric_calculators = _make_metric_calculators(metric_infos, model_names, classes_mapping)\n",
    "per_project_metric_calculators = [\n",
    "    _make_metric_calculators(metric_infos, model_names, classes_mapping) for _ in project_infos]\n",
    "\n",
    "def _get_name(x):\n",
    "    return x.name\n",
    "\n",
    "for project_idx, project_info in enumerate(project_infos):\n",
    "    meta_gt = sly.ProjectMeta.from_json(api.project.get_meta(project_info.id))\n",
    "    for model_idx, model_info in enumerate(model_infos):\n",
    "        inference_project_name = inference_project_names[project_idx, model_idx]\n",
    "        inference_project_info = api.project.get_info_by_name(workspace.id, inference_project_name)\n",
    "        meta_pred = sly.ProjectMeta.from_json(api.project.get_meta(inference_project_info.id))\n",
    "\n",
    "        datasets_gt = sorted(api.dataset.get_list(project_info.id), key=_get_name)\n",
    "        datasets_pred = sorted(api.dataset.get_list(inference_project_info.id), key=_get_name)\n",
    "\n",
    "        for ds_gt, ds_pred in zip(datasets_gt, datasets_pred):\n",
    "            images_gt = sorted(api.image.get_list(ds_gt.id), key=_get_name)\n",
    "            images_pred = sorted(api.image.get_list(ds_pred.id), key=_get_name)\n",
    "\n",
    "            for batch_gt, batch_pred in zip(sly.batched(images_gt), sly.batched(images_pred)):\n",
    "                image_gt_ids = [image_info.id for image_info in batch_gt]\n",
    "                image_pred_ids = [image_info.id for image_info in batch_pred]\n",
    "                ann_gt_infos = api.annotation.download_batch(ds_gt.id, image_gt_ids)\n",
    "                ann_pred_infos = api.annotation.download_batch(ds_pred.id, image_pred_ids)\n",
    "\n",
    "                for ann_gt_info, ann_pred_info in zip(ann_gt_infos, ann_pred_infos):\n",
    "                    ann_gt = sly.Annotation.from_json(ann_gt_info.annotation, meta_gt)\n",
    "                    ann_pred = sly.Annotation.from_json(ann_pred_info.annotation, meta_pred)\n",
    "                    for metric_idx in range(len(metric_infos)):\n",
    "                        overall_metric_calculators[model_idx][metric_idx].add_pair(ann_gt, ann_pred)\n",
    "                        per_project_metric_calculators[project_idx][model_idx][metric_idx].add_pair(ann_gt, ann_pred)\n",
    "\n",
    "\n",
    "def _print_metrics(metric_calculators, title_line=None):\n",
    "    total_metrics = [\n",
    "        [m.get_total_metrics() for m in model_metric_calculators]\n",
    "        for model_metric_calculators in metric_calculators]\n",
    "\n",
    "    table = PrettyTable(field_names=['Models'] + [\n",
    "        metric_title for metric_info in metric_infos for _, metric_title in metric_info.metric_id_with_title.items()])\n",
    "    for model_name, all_model_totals in zip(model_names, total_metrics):\n",
    "        model_values_nested = [[metric_totals[metric_id] for metric_id, _ in metric_info.metric_id_with_title.items()]\n",
    "                               for metric_info, metric_totals in zip(metric_infos, all_model_totals)]\n",
    "        model_values_flat = list(itertools.chain(*model_values_nested))\n",
    "        table.add_row([model_name] + model_values_flat)\n",
    "    if title_line is not None:\n",
    "        print(title_line)\n",
    "    print(table.get_string(), flush=True)\n",
    "\n",
    "_print_metrics(overall_metric_calculators, title_line='Overall metrics:')\n",
    "\n",
    "for project_info, project_metric_calculators in zip(project_infos, per_project_metric_calculators):\n",
    "    _print_metrics(project_metric_calculators, title_line=f'Metrics for project {project_info.name!r}:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
