{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import supervisely_lib as sly\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'http://192.168.1.69:5555'\n",
    "token = 'YGPDnuBkhFmcQ7VNzSEjhgavjg4eFR4Eq1C3jIY4HgV3SQq2JgkXCNtgZy1Fu2ftd4IKui8DsjrdtXjB853cMtBevpSJqFDYiaG1A5qphlH6fFiYYmcVZ5fMR8dDrt5l'\n",
    "team_name = 'dima'\n",
    "workspace_name = 'work'\n",
    "\n",
    "src_project_name = 'cls_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = sly.Api(address, token)\n",
    "\n",
    "team_id = api.team.get_info_by_name(team_name)['id']\n",
    "workspace_id = api.workspace.get_info_by_name(workspace_name, team_id)['id']\n",
    "\n",
    "src_project_id = api.project.get_info_by_name(src_project_name, workspace_id)['id']\n",
    "\n",
    "src_meta_json = api.project.get_meta(src_project_id)\n",
    "src_meta = sly.ProjectMeta.from_json(src_meta_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_mapping = {\n",
    "    \"HumanTrue\": \"Human\",\n",
    "    \"Human\": \"NoneTag\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_POSITIVE = 'true-positive'\n",
    "TRUE_NEGATIVE = 'true-negative'\n",
    "FALSE_POSITIVE = 'false-positive'\n",
    "FALSE_NEGATIVE = 'false-negative'\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "METRIC_PRECISION = 'precision'\n",
    "METRIC_RECALL = 'recall'\n",
    "METRIC_F1_MEASURE = 'F1-measure'\n",
    "METRICS_TOTAL = 'total'\n",
    "\n",
    "\n",
    "def classification_metrics(ann):\n",
    "    current_metric_res = {}\n",
    "    for cls_1, cls_2 in tags_mapping.items():\n",
    "        c1 = ann.img_tags.has_key(cls_1)\n",
    "        c2 = ann.img_tags.has_key(cls_2)\n",
    "\n",
    "        current_metric_res[(cls_1, cls_2)] = {\n",
    "            TRUE_POSITIVE: int(c1 and c2),\n",
    "            TRUE_NEGATIVE: int(not c1 and not c2),\n",
    "            FALSE_POSITIVE: int(not c1 and c2),\n",
    "            FALSE_NEGATIVE: int(c1 and not c2)\n",
    "        }\n",
    "    return current_metric_res\n",
    "\n",
    "\n",
    "def process(ann, metric_res, metric_total_res):    \n",
    "    res = classification_metrics(ann)\n",
    "    for pair_name, met_data in res.items():\n",
    "        for metric_name, metric_value in met_data.items():\n",
    "            metric_res[pair_name][metric_name] += metric_value\n",
    "            metric_total_res[metric_name] += metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 34.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project/Dataset: cls_test/ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric_res = defaultdict(lambda: {TRUE_POSITIVE: 0,\n",
    "                                  TRUE_NEGATIVE: 0,\n",
    "                                  FALSE_POSITIVE: 0,\n",
    "                                  FALSE_NEGATIVE: 0})\n",
    "\n",
    "metric_total_res = defaultdict(lambda: 0)\n",
    "\n",
    "for dataset_info in api.dataset.get_list(src_project_id):\n",
    "    src_dataset_id = dataset_info['id']\n",
    "    src_dataset_name = dataset_info['name']\n",
    "\n",
    "    print('Project/Dataset: {}/{}'.format(src_project_name, src_dataset_name))\n",
    "    \n",
    "    for image_info in tqdm(api.image.get_list(src_dataset_id)):\n",
    "        src_image_ext = image_info['meta']['mime'].split('/')[1]\n",
    "\n",
    "        ann_json = api.annotation.download(src_dataset_id, image_info['id'])\n",
    "        ann = sly.Annotation.from_json(ann_json, src_meta)\n",
    "        process(ann, metric_res, metric_total_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complex_metrics(values, pair_name):\n",
    "    TP = values[TRUE_POSITIVE]\n",
    "    TN = values[TRUE_NEGATIVE]\n",
    "    FP = values[FALSE_POSITIVE]\n",
    "    FN = values[FALSE_NEGATIVE]\n",
    "\n",
    "    values[METRIC_ACCURACY] = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    def zero_division_message(metric_name):\n",
    "        print('Metric \"{}\" for pair {} is 0, because denominator is 0'.format(metric_name, pair_name))\n",
    "\n",
    "    try:\n",
    "        values[METRIC_PRECISION] = TP / (TP + FP)\n",
    "    except ZeroDivisionError:\n",
    "        values[METRIC_PRECISION] = 0\n",
    "        zero_division_message(METRIC_PRECISION)\n",
    "\n",
    "    try:\n",
    "        values[METRIC_RECALL] = TP / (TP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        values[METRIC_RECALL] = 0\n",
    "        zero_division_message(METRIC_RECALL)\n",
    "\n",
    "    try:\n",
    "        values[METRIC_F1_MEASURE] = (2.0 * TP) / (2.0 * TP + FP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        values[METRIC_F1_MEASURE] = 0\n",
    "        zero_division_message(METRIC_F1_MEASURE)\n",
    "        \n",
    "for pair_name, values in metric_res.items():\n",
    "    calculate_complex_metrics(values, pair_name)\n",
    "calculate_complex_metrics(metric_total_res, \"(ALL, ALL)\")\n",
    "metric_res[METRICS_TOTAL] = metric_total_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total values:\n",
      "    accuracy        :   0.3750\n",
      "    precision       :   0.6000\n",
      "    recall          :   0.5000\n",
      "    F1-measure      :   0.5455\n",
      "    true-positive   :   3.0000\n",
      "    true-negative   :   0.0000\n",
      "    false-positive  :   2.0000\n",
      "    false-negative  :   3.0000\n",
      "1) ('HumanTrue', 'Human'):\n",
      "    accuracy        :   0.5000\n",
      "    precision       :   0.6667\n",
      "    recall          :   0.6667\n",
      "    F1-measure      :   0.6667\n",
      "    true-positive   :   2.0000\n",
      "    true-negative   :   0.0000\n",
      "    false-positive  :   1.0000\n",
      "    false-negative  :   1.0000\n",
      "\n",
      "2) ('Human', 'NoneTag'):\n",
      "    accuracy        :   0.2500\n",
      "    precision       :   0.5000\n",
      "    recall          :   0.3333\n",
      "    F1-measure      :   0.4000\n",
      "    true-positive   :   1.0000\n",
      "    true-negative   :   0.0000\n",
      "    false-positive  :   1.0000\n",
      "    false-negative  :   2.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_evaluation_values(values):\n",
    "    labels = ['accuracy', 'precision', 'recall', 'F1-measure',\n",
    "              'true-positive', 'true-negative', 'false-positive', 'false-negative']\n",
    "    for label in labels:\n",
    "        print('    {0}:   {1:2.4f}'.format(label.ljust(16), values[label]))\n",
    "\n",
    "print('Total values:')\n",
    "print_evaluation_values(metric_res[METRICS_TOTAL])\n",
    "\n",
    "counter = 0\n",
    "for key, value in metric_res.items():\n",
    "    if key == METRICS_TOTAL:\n",
    "        continue\n",
    "    counter += 1\n",
    "    print('{0}) {1}:'.format(counter, key))\n",
    "    print_evaluation_values(value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
