{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import supervisely_lib as sly\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'http://192.168.1.69:5555'\n",
    "token = 'YGPDnuBkhFmcQ7VNzSEjhgavjg4eFR4Eq1C3jIY4HgV3SQq2JgkXCNtgZy1Fu2ftd4IKui8DsjrdtXjB853cMtBevpSJqFDYiaG1A5qphlH6fFiYYmcVZ5fMR8dDrt5l'\n",
    "team_name = 'dima'\n",
    "workspace_name = 'work'\n",
    "\n",
    "src_project_name = 'roads_inf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = sly.Api(address, token)\n",
    "\n",
    "team_id = api.team.get_info_by_name(team_name)['id']\n",
    "workspace_id = api.workspace.get_info_by_name(workspace_name, team_id)['id']\n",
    "\n",
    "src_project_id = api.project.get_info_by_name(src_project_name, workspace_id)['id']\n",
    "\n",
    "src_meta_json = api.project.get_meta(src_project_id)\n",
    "src_meta = sly.ProjectMeta.from_json(src_meta_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_mapping = {\n",
    "    'road': 'road_unet'\n",
    "}\n",
    "iou_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(mask_1, mask_2):\n",
    "    intersection = (mask_1 * mask_2).sum()\n",
    "    union = mask_1.sum() + mask_2.sum() - intersection\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def compute_overlaps(masks1, masks2):\n",
    "    overlaps = np.zeros((masks1.shape[0], masks2.shape[0]))\n",
    "    for i in range(overlaps.shape[1]):\n",
    "        mask2 = masks2[i]\n",
    "        for j in range(overlaps.shape[0]):\n",
    "            mask1 = masks1[j]\n",
    "            overlaps[j, i] = iou(mask1, mask2)\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def compute_matches(gt_masks, pred_masks, iou_threshold):\n",
    "    overlaps = compute_overlaps(pred_masks, gt_masks)\n",
    "    pred_match = -1 * np.ones([pred_masks.shape[0]])\n",
    "    gt_match = -1 * np.ones([gt_masks.shape[0]])\n",
    "    for i in range(len(pred_masks)):\n",
    "        sorted_ixs = np.argsort(overlaps[i])[::-1]\n",
    "        for j in sorted_ixs:\n",
    "            if gt_match[j] > 0:\n",
    "                continue\n",
    "            iou = overlaps[i, j]\n",
    "            if iou < iou_threshold:\n",
    "                break\n",
    "            gt_match[j] = i\n",
    "            pred_match[i] = j\n",
    "            break\n",
    "\n",
    "    return gt_match, pred_match\n",
    "\n",
    "\n",
    "def compute_precision_recall(gt_masks, pred_masks, iou_threshold=0.5):\n",
    "    if len(gt_masks) == 0:\n",
    "        return 0, len(pred_masks), 0\n",
    "    elif len(pred_masks) == 0:\n",
    "        return 0, 0, len(gt_masks)\n",
    "    gt_masks = np.stack(gt_masks, axis=0)\n",
    "    pred_masks = np.stack(pred_masks, axis=0)\n",
    "    gt_match, pred_match = compute_matches(gt_masks, pred_masks, iou_threshold)\n",
    "\n",
    "    tp = np.sum(pred_match > -1)\n",
    "    all_pred = len(pred_match)\n",
    "\n",
    "    all_gt = len(gt_match)\n",
    "    return tp, all_pred, all_gt\n",
    "\n",
    "\n",
    "def process(ann, metric_res):\n",
    "    img_size = ann.img_size\n",
    "    for cls_gt, cls_pred in classes_mapping.items():\n",
    "        masks_gt, masks_pred = [], []\n",
    "        for label in ann.labels:\n",
    "            if label.obj_class.name == cls_gt:\n",
    "                mask = np.zeros(img_size, np.uint8)\n",
    "                label.geometry.draw(mask, 1)\n",
    "                masks_gt.append(mask)\n",
    "            if label.obj_class.name == cls_pred:\n",
    "                mask = np.zeros(img_size, np.uint8)\n",
    "                label.geometry.draw(mask, 1)\n",
    "                masks_pred.append(mask)\n",
    "                \n",
    "        tp, all_pred, all_gt = compute_precision_recall(masks_gt, masks_pred, iou_threshold)\n",
    "        pair_name = cls_gt + ':' + cls_pred\n",
    "        metric_res[pair_name]['true-positive'] += tp\n",
    "        metric_res[pair_name]['all-ground-truth'] += all_gt\n",
    "        metric_res[pair_name]['all-predictions'] += all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00, 26.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project/Dataset: roads_inf/ds1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 27.66it/s]\n"
     ]
    }
   ],
   "source": [
    "metric_results = defaultdict(lambda: {'true-positive': 0, 'all-ground-truth': 0, 'all-predictions': 0})\n",
    "for dataset_info in api.dataset.get_list(src_project_id):\n",
    "    src_dataset_id = dataset_info['id']\n",
    "    src_dataset_name = dataset_info['name']\n",
    "\n",
    "    print('Project/Dataset: {}/{}'.format(src_project_name, src_dataset_name))\n",
    "    \n",
    "    for image_info in tqdm(api.image.get_list(src_dataset_id)):\n",
    "        src_image_ext = image_info['meta']['mime'].split('/')[1]\n",
    "\n",
    "        ann_json = api.annotation.download(src_dataset_id, image_info['id'])\n",
    "        ann = sly.Annotation.from_json(ann_json, src_meta)\n",
    "        process(ann, metric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for classes 'road' and 'road_unet'.\n",
      "Precision: 0.8888888888888888\n",
      "Recall: 0.8\n",
      "\n",
      "Average for all classes.\n",
      "Precision: 0.8888888888888888\n",
      "Recall: 0.8\n"
     ]
    }
   ],
   "source": [
    "all_prec = []\n",
    "all_rec = []\n",
    "for cls_pair in metric_results:\n",
    "    met_vals = metric_results[cls_pair]\n",
    "    precision = met_vals['true-positive'] / met_vals['all-predictions'] if met_vals['all-predictions'] > 0 else 0\n",
    "    recall = met_vals['true-positive'] / met_vals['all-ground-truth'] if met_vals['all-ground-truth'] > 0 else 0\n",
    "    all_prec.append(precision)\n",
    "    all_rec.append(recall)\n",
    "    print('Results for classes {!r} and {!r}.'.format(*cls_pair.split(':')))\n",
    "    print('Precision: {}'.format(precision))\n",
    "    print('Recall: {}'.format(recall))\n",
    "    print()\n",
    "print('Average for all classes.')\n",
    "print('Precision: {}'.format(np.mean(all_prec)))\n",
    "print('Recall: {}'.format(np.mean(all_rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda: {\"a\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['a']['a'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {'a': {'a': 2}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
