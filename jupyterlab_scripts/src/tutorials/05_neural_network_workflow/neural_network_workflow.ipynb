{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervisely Tutorial #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks: training workflow with Supervisely online API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will show how use Supervisely online API to perform a full cycle of preparing neural network training data, train a model and then run inference on a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup steps\n",
    "\n",
    "Before we can start issuing inference requests, we need to connect to the Supervisely web instance, make sure the model we need is available and set up a worker machine to load the model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports\n",
    "\n",
    "Simply import the Supervisrly Python SDK module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervisely_lib as sly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize API access with your credentials\n",
    "\n",
    "Before starting to interact with a Supervisely web instance using our API, you need to supply your use credentials: your unique access token that you can find under your profile details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server address:  http://192.168.1.69:5555\n",
      "Your API token:  OfaV5z24gEQ7ikv2DiVdYu1CXZhMavU7POtJw2iDtQtvGUux31DUyWTXW6mZ0wd3IRuXTNtMFS9pCggewQWRcqSTUi4EJXzly8kH7MJL1hm3uZeM2MCn5HaoEYwXejKT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Jupyter notebooks hosted on Supervisely can get their user's\n",
    "# credentials from the environment varibales.\n",
    "# If you are running the notebook outside of Supervisely, plug\n",
    "# the server address and your API token here.\n",
    "# You can find your API token in the account settings:\n",
    "# -> click your name in the top-right corner\n",
    "# -> select \"account settings\"\n",
    "# -> select \"API token\" tab on top.\n",
    "address = os.environ['SERVER_ADDRESS']\n",
    "token = os.environ['API_TOKEN']\n",
    "\n",
    "print(\"Server address: \", address)\n",
    "print(\"Your API token: \", token)\n",
    "\n",
    "# Initialize the API access object.\n",
    "api = sly.Api(address, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the active workspace\n",
    "\n",
    "In Supervisely, every neural network model (and also every data project) is stored in a context of a certain *workspace*. See our tutorial #2 for a detailed guide on how to work with workspaces using our online API.\n",
    "\n",
    "Here we will create a new workspace to avoid interfering with any existing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: id=9, name=max\n",
      "Workspace: id=116, name=api_training_tutorial\n"
     ]
    }
   ],
   "source": [
    "# In Supervisely, a user can belong to multiple teams.\n",
    "# Everyone has a default team with just their user in it.\n",
    "# We will work in the context of that default team.\n",
    "team = api.team.get_list()[0]\n",
    "\n",
    "# Set up the name of a new workspace to be created.\n",
    "workspace_name = \"api_training_tutorial\"\n",
    "\n",
    "# Just in case there is already a workspace with this name,\n",
    "# we can ask the web instance for a new unique name to use.\n",
    "if api.workspace.exists(team.id, workspace_name):\n",
    "    workspace = api.workspace.get_info_by_name(team.id, workspace_name)\n",
    "else:\n",
    "    workspace = api.workspace.create(team.id, workspace_name)\n",
    "\n",
    "# Print out the results.\n",
    "# Here we will see which name our workspace ended up with.\n",
    "print(\"Team: id={}, name={}\".format(team.id, team.name))\n",
    "print(\"Workspace: id={}, name={}\".format(workspace.id, workspace.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a neural network to the workspace to be a starting point for training\n",
    "\n",
    "Training neural networks completely from scratch is done very rarely in practice. Instead, one uses an existing model, trained on a large dataset like ImageNet, as a starting point, and fine-tunes the weights for the specific task at hand. This way most of the initial network layers, extracting low-level features, can be reused almost without change, since low-level features typically transfer well between different tasks.\n",
    "\n",
    "In this tutorial we will be basing our model on UNet trained on ImageNet dataset. This model is publically available out of the box in Supervisely, so we only need to clone it into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: id = 362, name = 'unet_vgg'\n"
     ]
    }
   ],
   "source": [
    "# Set the destination model name within our workspace\n",
    "model_name = \"unet_vgg\"\n",
    "\n",
    "# Grab a unique name in case the one we chose initially is busy.\n",
    "if api.model.exists(workspace.id, model_name):\n",
    "    model_name = api.model.get_free_name(workspace.id, model_name)\n",
    "\n",
    "# Request the model to be copied from our public repository.\n",
    "# This kicks off an asynchronous task.\n",
    "task_id = api.model.clone_from_explore('Supervisely/Model Zoo/UNet (VGG weights)', workspace.id, model_name)\n",
    "\n",
    "# Wait for the copying to complete.\n",
    "api.task.wait(task_id, api.task.Status.FINISHED)\n",
    "\n",
    "# Query the metadata for the copied model.\n",
    "model = api.model.get_info_by_name(workspace.id, model_name)\n",
    "print(\"Model: id = {}, name = {!r}\".format(model.id, model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the agent to use\n",
    "\n",
    "Neural network inference is a computationally intensive process, so it is infeasible to have the inference run on the same machine that serves the Supervisely web instance. Instead, you need to connect a worker machine (with a GPU) to the web instance to run the computations. The worker is connected using the *Supervisely Agent* - an open-source daemon that runs on the worker, connects to the web instance and listens for tasks to execute. See https://github.com/supervisely/supervisely/tree/master/agent for details on how to run the agent.\n",
    "\n",
    "From now on the tutorial assumes that you have launched the agent on your worker machine and it shows up on your \"Cluster\" page in the Supervisely web instance. We first query the instance for the agent ID by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your agent name. You can find the list of\n",
    "# all your agents in the \"Cluster\" menu in the Supervisely instance.\n",
    "agent_name = \"agent_01\"\n",
    "\n",
    "agent = api.agent.get_info_by_name(team.id, agent_name)\n",
    "if agent is None:\n",
    "    raise RuntimeError(\"Agent {!r} not found\".format(agent_name))\n",
    "if agent.status is api.agent.Status.WAITING:\n",
    "    raise RuntimeError(\"Agent {!r} is not running\".format(agent_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a project with the training data\n",
    "\n",
    "Here we will take a small project with labeled data, apply augmentations to it to increase the data variability and produce the resuling project with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy an existing labeled project\n",
    "\n",
    "We will clone one of the publically available in Supervisely projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: id = 1279, name = 'lemons_annotated_copy'\n"
     ]
    }
   ],
   "source": [
    "project_annotated_name = \"lemons_annotated_copy\"\n",
    "\n",
    "# Grab a free project name if ours is taken.\n",
    "if api.project.exists(workspace.id, project_annotated_name):\n",
    "    project_annotated_name = api.project.get_free_name(workspace.id, project_annotated_name)\n",
    "\n",
    "# Kick off the a project clone task and wait for completion.\n",
    "task_id = api.project.clone_from_explore('Supervisely/Demo/lemons_annotated', workspace.id, project_annotated_name)\n",
    "api.task.wait(task_id, api.task.Status.FINISHED)\n",
    "\n",
    "src_project = api.project.get_info_by_name(workspace.id, project_annotated_name)\n",
    "print(\"Project: id = {}, name = {!r}\".format(src_project.id, src_project.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation using a Data Transformation Language plugin\n",
    "\n",
    "To increase variaility of the training data and make the learned models more robust, a common approach is to perform *augmentations*. Augmentations are image transformations that preserve the essential appearance of the objects, like random crops or rotations. The transformed data is then used as training data for the model.\n",
    "\n",
    "In Supervisely, one way to conveniently augment your data is to run a Data Transformation Language plugin with a set of predefined transformtations. The output of the plugin forms a new project, which we will then use for training the model. The DTL workflow will also tag images as `train` and `val` to distinguish between training and validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTL task (id=2208) is started\n",
      "Training dataset 'lemons_train' contains 72 images\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "project_train_name = \"lemons_train\"\n",
    "\n",
    "# read graph template and define input/output projects\n",
    "with open('./dtl_segmentation_graph.json', 'r') as file:\n",
    "    dtl_graph_str = file.read()\n",
    "\n",
    "# Plug in the source and destination project names in\n",
    "# the DTL transformations definition.\n",
    "dtl_graph_str = dtl_graph_str.replace('%SRC_PROJECT_NAME%', project_annotated_name)\n",
    "dtl_graph_str = dtl_graph_str.replace('%DST_PROJECT_NAME%', project_train_name)\n",
    "\n",
    "# Parse the JSON data with filled in project names.\n",
    "dtl_graph = json.loads(dtl_graph_str)\n",
    "\n",
    "# Run the DTL transformation unless we already have the output project\n",
    "# (say in case you are executing this notebook multiple times).\n",
    "task_id = None\n",
    "if not api.project.exists(workspace.id, project_train_name):\n",
    "    # Kick off asynchronous DTL task.\n",
    "    task_id = api.task.run_dtl(workspace.id, dtl_graph, agent.id)\n",
    "    print('DTL task (id={}) is started'.format(task_id))\n",
    "    \n",
    "    # Wait for the task to complete.\n",
    "    if task_id is not None:\n",
    "        api.task.wait(task_id, api.task.Status.FINISHED)\n",
    "        \n",
    "# Inspect the results.\n",
    "project_train = api.project.get_info_by_name(workspace.id, project_train_name)\n",
    "print(\"Training dataset {!r} contains {} images\".format(\n",
    "    project_train.name,\n",
    "    api.project.get_images_count(project_train.id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run neural network training\n",
    "\n",
    "We are ready to fine-tune the original UNet model to find lemons and kiwis from our toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train task (id=2209) is started\n"
     ]
    }
   ],
   "source": [
    "# Select a name for the trained model.\n",
    "trained_model_name = \"nn_lemon_kiwi\"\n",
    "\n",
    "# Fill in the training config.\n",
    "training_config = {\n",
    "  \"lr\": 0.001,       # Learning rate\n",
    "  \"epochs\": 10,      # Number of epochs (full iteration over training data) to use.\n",
    "  \"val_every\": 0.5,  # Validate every 0.5 epoch (i.e. twice per epoch).\n",
    "  \"input_size\": {    # Resize input images to this size before feeding the neural net.\n",
    "    \"width\": 256,\n",
    "    \"height\": 256\n",
    "  },\n",
    "  \"gpu_devices\": [ 0 ],  # Use the first available GPU for training.\n",
    "  \"dataset_tags\": {  # Use the images tagged as 'train' for training and 'val' for validation.\n",
    "    \"val\": \"val\",\n",
    "    \"train\": \"train\"\n",
    "  },\n",
    "  \"special_classes\": {\n",
    "    \"background\": \"bg\"\n",
    "  },\n",
    "  # Transfer learning means we are not reusing the set of classes that the initial\n",
    "  # model, and instead introducing a new set of classes, so the head (last layer)\n",
    "  # of the model should be reinitialized.\n",
    "  \"weights_init_type\": \"transfer_learning\"\n",
    "}\n",
    "\n",
    "task_id = api.task.run_train(agent.id, project_train.id, model.id, trained_model_name, training_config)\n",
    "print('Train task (id={}) is started'.format(task_id))\n",
    "\n",
    "api.task.wait(task_id, api.task.Status.FINISHED)\n",
    "print('Train task (id={}) is finished'.format(task_id))\n",
    "\n",
    "trained_model = api.model.get_info_by_name(workspace.id, trained_model_name)\n",
    "if trained_model is None:\n",
    "    raise RuntimeError(\"Model {!r} not found\".format(trained_model_name))\n",
    "\n",
    "print(\"trained model: id = {}, name = {!r}\".format(trained_model.id, trained_model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on the freshly trained model\n",
    "\n",
    "Now we can run inference with the learned model. Here we only show a minimal overview. See Supervisely tutorial #4 for more details on online inference API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare an input dataset for inference\n",
    "\n",
    "We will clone a publically available unlabeled project to run inference on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: id = 1106, name = 'lemons_test_copy'\n"
     ]
    }
   ],
   "source": [
    "project_test_name = \"lemons_test_copy\"\n",
    "\n",
    "# Grab a free project name if ours is taken.\n",
    "if api.project.exists(workspace.id, project_test_name):\n",
    "    project_test_name = api.project.get_free_name(workspace.id, project_test_name)\n",
    "\n",
    "# Kick off the a project clone task and wait for completion.\n",
    "task_id = api.project.clone_from_explore('Supervisely/Demo/lemons_test', workspace.id, project_test_name)\n",
    "api.task.wait(task_id, api.task.Status.FINISHED)\n",
    "\n",
    "project_test = api.project.get_info_by_name(workspace.id, project_test_name)\n",
    "print(\"Project: id = {}, name = {!r}\".format(project_test.id, project_test.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on the input project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference task (id=1948) is started\n",
      "Inference task (id=1948) is finished\n"
     ]
    }
   ],
   "source": [
    "# Name of destination project with the test results.\n",
    "project_inf_name = \"lemons_test_inf\"\n",
    "\n",
    "task_id = api.task.run_inference(agent.id, project_test.id, trained_model.id, project_inf_name)\n",
    "print('Inference task (id={}) is started'.format(task_id))\n",
    "\n",
    "api.task.wait(task_id, api.task.Status.FINISHED)\n",
    "print('Inference task (id={}) is finished'.format(task_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
